import zlib
from collections.abc import Iterable
from typing import cast

import ctranslate2
import numpy as np
from faster_whisper.tokenizer import Tokenizer
from faster_whisper.vad import SpeechTimestampsMap

from eavesdrop.server.transcription.models import SpeechChunk, WordTimingDict
from eavesdrop.wire import Segment


def restore_speech_timestamps(
  segments: Iterable[Segment],
  speech_chunks: list[SpeechChunk],
  sampling_rate: int,
) -> Iterable[Segment]:
  ts_map = SpeechTimestampsMap(cast(list[dict], speech_chunks), sampling_rate)

  for segment in segments:
    if segment.words:
      words = []
      for word in segment.words:
        # Ensure the word start and end times are resolved to the same chunk.
        middle = (word.start + word.end) / 2
        chunk_index = ts_map.get_chunk_index(middle)
        word.start = ts_map.get_original_time(word.start, chunk_index)
        word.end = ts_map.get_original_time(word.end, chunk_index)
        words.append(word)

      segment.start = words[0].start
      segment.end = words[-1].end
      segment.words = words

    else:
      segment.start = ts_map.get_original_time(segment.start)
      segment.end = ts_map.get_original_time(segment.end)
  return segments


def get_ctranslate2_storage(segment: np.ndarray) -> ctranslate2.StorageView:
  segment = np.ascontiguousarray(segment)
  segment = ctranslate2.StorageView.from_array(segment)
  return segment


def get_compression_ratio(text: str) -> float:
  text_bytes = text.encode("utf-8")
  return len(text_bytes) / len(zlib.compress(text_bytes))


def get_suppressed_tokens(
  tokenizer: Tokenizer,
  suppress_tokens: list[int],
) -> list[int]:
  if -1 in suppress_tokens:
    suppress_tokens = [t for t in suppress_tokens if t >= 0]
    suppress_tokens.extend(tokenizer.non_speech_tokens)
  elif suppress_tokens is None or len(suppress_tokens) == 0:
    suppress_tokens = []  # interpret empty string as an empty list

  suppress_tokens.extend(
    [
      tokenizer.transcribe,
      tokenizer.translate,
      tokenizer.sot,
      tokenizer.sot_prev,
      tokenizer.sot_lm,
    ]
  )

  return list(sorted(set(suppress_tokens)))


def merge_punctuations(alignment: list[WordTimingDict], prepended: str, appended: str) -> None:
  # merge prepended punctuations
  i = len(alignment) - 2
  j = len(alignment) - 1
  while i >= 0:
    previous = alignment[i]
    following = alignment[j]
    if previous["word"].startswith(" ") and previous["word"].strip() in prepended:
      # prepend it to the following word
      following["word"] = previous["word"] + following["word"]
      following["tokens"] = previous["tokens"] + following["tokens"]
      previous["word"] = ""
      previous["tokens"] = []
    else:
      j = i
    i -= 1

  # merge appended punctuations
  i = 0
  j = 1
  while j < len(alignment):
    previous = alignment[i]
    following = alignment[j]
    if not previous["word"].endswith(" ") and following["word"] in appended:
      # append it to the previous word
      previous["word"] = previous["word"] + following["word"]
      previous["tokens"] = previous["tokens"] + following["tokens"]
      following["word"] = ""
      following["tokens"] = []
    else:
      i = j
    j += 1
